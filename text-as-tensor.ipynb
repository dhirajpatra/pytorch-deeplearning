{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cbc40f-79ae-4359-8ded-7bb4e35ebfa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Representing text\n",
    "\n",
    "If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors. Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8.\n",
    "\n",
    "<img alt=\"Image showing diagram mapping a character to an ASCII and binary representation\" src=\"images/2-represent-text-as-tensor-checkpoint-1.png\" align=\"middle\" />\n",
    "\n",
    "We understand what each letter **represents**, and how all characters come together to form the words of a sentence. However, computers by themselves do not have such an understanding, and a neural network has to learn the meaning during training.\n",
    "\n",
    "Therefore, we can use different approaches when representing text:\n",
    "* **Character-level representation**, when we represent text by treating each character as a number. Given that we have $C$ different characters in our text corpus, the word *Hello* would be represented by $5\\times C$ tensor. Each letter would correspond to a tensor column in one-hot encoding.\n",
    "* **Word-level representation**, when we create a **vocabulary** of all words in our text sequence or sentence(s), and then represent each word using one-hot encoding. This approach is somehow better, because each letter by itself does not have much meaning, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network. However, given a large dictionary size, we need to deal with high-dimensional sparse tensors.  For example, if we have a vocabulary size of 10,000 different words.  Then each word would have an one-hot encoding length of 10,000; hence the high-dimensional.\n",
    "\n",
    "To unify those approaches, we typically call an atomic piece of text **a token**. In some cases tokens can be letters, in other cases - words, or parts of words.\n",
    "\n",
    "> For example, we can choose to tokenize *indivisible* as `in`-`divis`-`ible`, where the `#` sign represents that the token is a continuation of the previous word. This would allow the root `divis` to always be represented by one token, corresponding to one core meaning.\n",
    "\n",
    "The process of converting text into a sequence of tokens is called **tokenization**. Next, we need to assign each token to a number, which we can feed into a neural network. This is called **vectorization**, and is normally done by building a token vocabulary.  \n",
    "\n",
    "Let's start by installing some required Python packages we'll use in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e828009-4a3f-417b-9f0a-57da7d3b129c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa85767-55b0-41e3-af7d-672c2243f7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'data' directory exists\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Load AG_NEWS dataset\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# Create an iterator for the train_dataset\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "# Example: Print the first item\n",
    "print(next(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c4f779-b801-4e2f-852c-c82512b5f17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Business** -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "**Business** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "\n",
      "**Business** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "\n",
      "**Business** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "\n",
      "**Business** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load AG_NEWS dataset\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# Iterate over the first 5 samples in the train_dataset\n",
    "for i, (label, text) in zip(range(5), train_dataset):\n",
    "    print(f\"**{classes[label - 1]}** -> {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826b7d16-c362-4ed7-91e3-773aeeed91a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "730702fc-652b-4f38-85a4-f590f23c404b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aa23f38-54c3-418a-b4bf-a9bba2d43854",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first token list:\n",
      "['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short-sellers', ',', 'wall', 'street', \"'\", 's', 'dwindling\\\\band', 'of', 'ultra-cynics', ',', 'are', 'seeing', 'green', 'again', '.']\n",
      "\n",
      "second token list:\n",
      "['carlyle', 'looks', 'toward', 'commercial', 'aerospace', '(', 'reuters', ')', 'reuters', '-', 'private', 'investment', 'firm', 'carlyle', 'group', ',', '\\\\which', 'has', 'a', 'reputation', 'for', 'making', 'well-timed', 'and', 'occasionally\\\\controversial', 'plays', 'in', 'the', 'defense', 'industry', ',', 'has', 'quietly', 'placed\\\\its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market', '.']\n"
     ]
    }
   ],
   "source": [
    "first_sentence = train_dataset[0][1]\n",
    "second_sentence = train_dataset[1][1]\n",
    "\n",
    "f_tokens = tokenizer(first_sentence)\n",
    "s_tokens = tokenizer(second_sentence)\n",
    "\n",
    "print(f'\\nfirst token list:\\n{f_tokens}')\n",
    "print(f'\\nsecond token list:\\n{s_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6944f97-51fe-42d3-aab7-e06fa09e85d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip:   1%|â–‹                                                                                                       | 5.88M/862M [02:29<11:01:41, 21.6kB/s]"
     ]
    }
   ],
   "source": [
    "# Assuming you have defined your tokenizer function\n",
    "def tokenizer(line):\n",
    "    # Your tokenizer logic here\n",
    "    pass\n",
    "\n",
    "# Count tokens in the dataset\n",
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "\n",
    "# Create a vocabulary with a minimum frequency of 1\n",
    "vocab = torchtext.vocab.Vocab(counter)\n",
    "\n",
    "# Load GloVe embeddings (adjust the vectors parameter accordingly)\n",
    "glove_vectors = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "# Associate GloVe vectors with the vocabulary\n",
    "vocab.set_vectors(glove_vectors.stoi, glove_vectors.vectors, glove_vectors.dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd188e-9de2-4592-99e2-3a7c22018446",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup = [list((vocab[w], w)) for w in f_tokens]\n",
    "print(f'\\nIndex lockup in 1st sentence:\\n{word_lookup}')\n",
    "\n",
    "word_lookup = [list((vocab[w], w)) for w in s_tokens]\n",
    "print(f'\\nIndex lockup in 2nd sentence:\\n{word_lookup}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d7af8-6252-4520-a2ea-1672ac0cc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "def encode(x):\n",
    "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "vec = encode(first_sentence)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b352e-c9cf-4bea-9bac-aee1c737b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    return [vocab.itos[i] for i in x]\n",
    "\n",
    "decode(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7cccd-8e1f-4410-9e3b-a3b585817c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import ngrams_iterator\n",
    "\n",
    "bi_counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    bi_counter.update(ngrams_iterator(tokenizer(line),ngrams=2))\n",
    "bi_vocab = torchtext.vocab.Vocab(bi_counter, min_freq=2)\n",
    "\n",
    "print(f\"Bigram vocab size = {len(bi_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e1a38-4962-4b28-9de0-7c540c5e7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return [bi_vocab.stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode(first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2b4ce-86ac-443b-8913-61283f9fcb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
