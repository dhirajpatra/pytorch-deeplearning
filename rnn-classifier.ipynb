{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fdfada-6ca7-486f-9283-50f1e716c9f0",
   "metadata": {},
   "source": [
    "The hidden cell containing the current and prior state is calculated with the following formula:\n",
    "\n",
    "h(t)=tanh(W \n",
    "h\n",
    "​\n",
    " h \n",
    "t−1\n",
    "​\n",
    " +W \n",
    "x\n",
    "​\n",
    " x \n",
    "t\n",
    "​\n",
    " +B \n",
    "h\n",
    "​\n",
    " )\n",
    "y(t)=W \n",
    "y\n",
    "​\n",
    " h \n",
    "t\n",
    "​\n",
    " +B \n",
    "y\n",
    "​\n",
    " \n",
    "Tanh is hyperbolic tangent function, which is defined as tanh(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "−x\n",
    " \n",
    "e \n",
    "x\n",
    " −e \n",
    "−x\n",
    " \n",
    "​\n",
    " \n",
    "At each network block, weights W \n",
    "x\n",
    "​\n",
    "  are applied to the numeric word vector input value; applying the previous hidden state W \n",
    "h\n",
    "​\n",
    " ; and the final state W \n",
    "y\n",
    "​\n",
    " . The tanh activation function is applied to the hidden layer to produce values between [−1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed06a7-1e9d-4f24-aeef-18dd10e85037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchinfo import summary\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cc632-d6b3-4e01-8287-8748af6d863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,h = self.rnn(x)\n",
    "        return self.fc(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3156ecb8-d137-4805-88d4-9b29b0cfd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)\n",
    "net = RNNClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31990160-41b2-4bcc-ab02-9d7e7af35cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'class map: {classes}')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee0848-2cf0-439f-8a8b-f3af8fc9aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (target, data) in enumerate(test_loader):\n",
    "        \n",
    "        word_lookup = [vocab.itos[w] for w in data[batch_idx]]\n",
    "        unknow_vals = {'<unk>'}\n",
    "        word_lookup = [ele for ele in word_lookup if ele not in unknow_vals]\n",
    "        print('Input text:\\n {}\\n'.format(word_lookup))\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        pred = net(data)\n",
    "        print(torch.argmax(pred[batch_idx]))\n",
    "        print(\"Actual:\\nvalue={}, class_name= {}\\n\".format(target[batch_idx], classes[target[batch_idx]]))\n",
    "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(pred[0].argmax(0),classes[pred[0].argmax(0)]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6efa4f-e00b-46e5-9f87-a0b94adbf668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,(h,c) = self.rnn(x)\n",
    "        return self.fc(h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc1aa1-f6a2-4eed-87e6-e71112549ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cac0d7-9b85-4a23-a89d-0856e3304157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_length(b):\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # compute max length of a sequence in this minibatch and length sequence itself\n",
    "    len_seq = list(map(len,v))\n",
    "    l = max(len_seq)\n",
    "    return ( # tuple of three tensors - labels, padded features, length sequence\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v]),\n",
    "        torch.tensor(len_seq)\n",
    "    )\n",
    "\n",
    "train_loader_len = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=pad_length, shuffle=True)\n",
    "test_loader_len = torch.utils.data.DataLoader(test_dataset, batch_size=16, collate_fn=pad_length, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a8b65-73ee-48d6-93b6-d66f9042b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPackClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        pad_x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=True,enforce_sorted=False)\n",
    "        _,(h,c) = self.rnn(pad_x)\n",
    "        return self.fc(h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f4adb-eb0c-4668-a516-fcb8d38ce2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMPackClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch_emb(net,train_loader_len, lr=0.001,use_pack_sequence=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b24538-776b-4e70-aed7-3194b7597c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for label,text,off in test_loader_len:\n",
    "        \n",
    "        text, label = text.to(device), label.to(device)\n",
    "        off = off.to('cpu')\n",
    "        print(f'off value: {off}')\n",
    "        pred = net(text, off )\n",
    "        print(f'target {label}')\n",
    "        y=torch.argmax(pred, dim=1)\n",
    "        print(f'pred: {y}')\n",
    "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(y[0],classes[y[0]]))\n",
    "        print(\"Target:\\nvalue={}, class_name= {}\\n\".format(label[0],classes[label[0]]))\n",
    "        break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7a54f-c9cf-4314-8afd-b7d30bf5a294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
